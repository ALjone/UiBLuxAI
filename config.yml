#Training
total_timesteps: 10000000

#PPO
num_steps_per_env: 64
num_minibatches: 2
lr: 1.0e-3 #0.00025
gamma: 0.98
epochs_per_batch: 4
eps_clip: 0.1
lmbda: 0.95
ent_coef: 0.001
vf_coef: 1
max_grad_norm: 0.5
use_target_kl: True
target_kl_max: 0.3
target_kl_min : 0
anneal_lr: True
norm_adv: True
clip_loss_value: False #According to 37 PPO implementation details, this should be false
mode_or_sample: sample #sample

#Parallelization
parallel_envs: 16


#Environment
map_size: 32
n_factories_min: 2
n_factories_max: 2
max_game_length: 2000

self_play: False
keep_opponent_alive: True

#Architecture - Actor
kernel_size: 5
actor_n_blocks: 8
actor_intermediate_channels: 64
actor_use_batch_norm: False

#Architecture - Critic
critic_n_blocks: 12
critic_intermediate_channels: 32

#Teacher stuff:
KL_factor: 0
KL_steps_to_anneal: 5000000
teacher_path: smaller_model_checkpoint.t

#Other 
device: cuda #cuda, cpu, mps
path: None #most_recent.t
save_path: bigger_model.t #model.t
log_to_wb: True
log_rate: 20000 #Log every 100k steps ish


#Reward
light_reward: 0.1 #0.05
heavy_reward: 1
rubble_reward: 0.005
scaling_ice: 0.01 #Value of ice
scaling_ore: 0.01
scaling_water: 0.02
scaling_metal: 0.02

factory_lost: 0 #0.5
scaling_win: 0
end_of_game_reward: 0 #50
