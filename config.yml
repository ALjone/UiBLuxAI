#Training
max_episodes: 10000000000000
print_freq: 100 # print avg reward in the interval (in episodes)

#PPO
num_steps_per_env: 128
num_minibatches: 8  
lr: 0.00025
gamma: 0.999
epochs_per_batch: 2
eps_clip: 0.2
lmbda: 0.95
ent_coef: 0.005
vf_coef: 0.5
max_grad_norm: 0.5
use_target_kl: True
target_kl_max: 0.03
target_kl_min : 0
anneal_lr: True
norm_adv: True
clip_loss_value: True #According to 37 PPO implementation details, this should be false
mode_or_sample: sample #sample

#Parallelization
parallel_envs: 16


#Environment
map_size: 48
n_factories: 4

#Architecture - Actor
actor_n_blocks: 16
actor_intermediate_channels: 32

#Architecture - Critic
critic_n_blocks: 12
critic_intermediate_channels: 32

#Factory placement
water_weight: 2
ore_weight: 1

#Other 
device: cuda #cuda, cpu, mps
path: trained_3_days.t #model.t
save_path: bigger_model.t #model.t
video_save_rate: 1000 #How often to save a video, in episodes
log_to_wb: True
log_rate: 2000 #Log every 100k steps ish


#Reward
unit_punishment: 0 #0.05
rubble_reward: 0.005
scaling_ice: 0 #0.005 #Value of ice
scaling_ore: 0 #0.005
scaling_water_made: 0.01
scaling_metal_made: 0.01

factory_lost: 3


lichen_divide_value: 20000
scaling_lichen: 5
scaling_win: 0
